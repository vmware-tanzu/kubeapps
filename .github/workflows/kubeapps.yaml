# Copyright 2022 VMware, Inc.
# SPDX-License-Identifier: Apache-2.0

name: Kubeapps general pipeline

on:
  push:
    branches:
      - main
      - migrate-ci
  pull_request:
    branches:
      - main

env:
  DOCKER_VERSION: "20.10.14"
  DOCKER_REGISTRY_VERSION: "2.8.1"
  GOLANG_VERSION: "1.18.4"
  HELM_VERSION_MIN: "v3.2.0"
  HELM_VERSION_STABLE: "v3.9.2"
  GITHUB_VERSION: "2.14.2"
  IMAGES_TO_PUSH: "apprepository-controller dashboard asset-syncer pinniped-proxy kubeapps-apis"
  # IMG_DEV_TAG is the tags used for the Kubeapps docker images. Ideally there should be an IMG_PROD_TAG
  # but its value is dynamic and GitHub actions doesn't support it in the `env` block, so it is generated
  # as an output of the `setup` job.
  IMG_DEV_TAG: "build-${{ github.sha }}"
  # Apart from using a dev tag we use a different image ID to avoid polluting the tag
  # history of the production tag
  IMG_MODIFIER: "-ci"
  IMG_PREFIX: "kubeapps/"
  # We use IMG_PREFIX_FOR_FORKS for development purposes, it's used when the workflow is run from a fork of the kubeapps repo
  IMG_PREFIX_FOR_FORKS: "beni0888/"
#  IMG_PLATFORMS: "linux/amd64, linux/arm64"
  IMG_PLATFORMS: "linux/amd64"
  K8S_KIND_VERSION: "v1.22.9@sha256:8135260b959dfe320206eb36b3aeda9cffcb262f4b44cda6b33f7bb73f453105"
  KIND_VERSION: "v0.14.0"
  KUBECTL_VERSION: "v1.22.12"
  MKCERT_VERSION: "v1.4.4"
  NODE_VERSION: "16.16.0"
  OLM_VERSION: "v0.22.0"
  POSTGRESQL_VERSION: "14.4.0-debian-11-r13"
  RUST_VERSION: "1.62.0"
  SEMVER_VERSION: "3.3.0"

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      img_modifier: ${{ steps.set-outputs.outputs.img_modifier }}
      img_prefix: ${{ steps.set-outputs.outputs.img_prefix }}
      img_dev_tag: ${{ steps.set-outputs.outputs.img_dev_tag }}
      img_prod_tag: ${{ steps.set-outputs.outputs.img_prod_tag }}
      postgresql_version: ${{ steps.set-outputs.outputs.postgresql_version }}
      rust_version: ${{ steps.set-outputs.outputs.rust_version }}
      triggered_from_fork: ${{ steps.set-outputs.outputs.triggered_from_fork }}
    steps:
      - name: Set outputs
        id: set-outputs
        run: |
          if [[ ${GITHUB_REPOSITORY} == "vmware-tanzu/kubeapps" ]]; then
            echo "::set-output name=triggered_from_fork::false"
            echo "::set-output name=img_prefix::${IMG_PREFIX}"
          else
            echo "::set-output name=triggered_from_fork::true"
            # When running from forks, we push the images to a personal namespace (if configured)
            echo "::set-output name=img_prefix::${IMG_PREFIX_FOR_FORKS}"
          fi;
          
          if [[ ${GITHUB_REF_TYPE} == "tag" ]]; then
            echo "::set-output name=img_prod_tag::${GITHUB_REF_NAME}"
          else
            echo "::set-output name=img_prod_tag::latest"
          fi;          

          echo "::set-output name=img_modifier::${IMG_MODIFIER}"
          echo "::set-output name=img_dev_tag::${IMG_DEV_TAG}"
          echo "::set-output name=postgresql_version::${POSTGRESQL_VERSION}"
          echo "::set-output name=rust_version::${RUST_VERSION}"
      - name: Show outputs
        run: |
          echo "IMG_MODIFIER: ${{steps.set-outputs.outputs.img_modifier}}"
          echo "IMG_PREFIX: ${{steps.set-outputs.outputs.img_prefix}}"
          echo "IMG_DEV_TAG: ${{steps.set-outputs.outputs.img_dev_tag}}"
          echo "IMG_PROD_TAG: ${{steps.set-outputs.outputs.img_prod_tag}}"
          echo "POSTGRESQL_VERSION: ${{steps.set-outputs.outputs.postgresql_version}}"
          echo "RUST_VERSION: ${{steps.set-outputs.outputs.rust_version}}"
          echo "TRIGGERED_FROM_FORK: ${{steps.set-outputs.outputs.triggered_from_fork}}"

  test_go:
    needs:
      - setup
    runs-on: ubuntu-latest
    services:
      postgresql:
        image: bitnami/postgresql:${{needs.setup.outputs.postgresql_version}}
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
        env:
          ALLOW_EMPTY_PASSWORD: "yes"
    steps:
    - uses: actions/checkout@v3
    - name: Set up Go
      uses: actions/setup-go@v3
      with:
        go-version: ${{ env.GOLANG_VERSION }}
    - name: Run go unit tests
      run: make test
    - run: make test-db

  test_dashboard:
    runs-on: ubuntu-latest
    needs:
      - setup
    env:
      # Note that the max old space setting is per worker, so running the tests
      # with 4 workers on a 4Gb (free plan) needs 1Gb of max old space. Forcing
      # garbage collection to start earlier with 512M per worker.
      NODE_OPTIONS: "--max-old-space-size=512"
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: ${{ env.NODE_VERSION }}
      - name: Install dashboard dependencies
        run: yarn install --cwd=dashboard --frozen-lockfile
      - name: Run dashboard linter
        run: yarn --cwd=dashboard run lint
      - name: Run dashboard unit tests
        run: yarn --cwd=dashboard run test --maxWorkers=4 --coverage --logHeapUsage

  test_pinniped_proxy:
    needs:
      - setup
    runs-on: ubuntu-latest
    container:
      image: rust:${{needs.setup.outputs.rust_version}}
    steps:
      - uses: actions/checkout@v3
      - name: Run rust unit tests
        run: cargo test --manifest-path cmd/pinniped-proxy/Cargo.toml

  test_chart_render:
    needs:
      - setup
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-go@v3
        with:
          go-version: ${{ env.GOLANG_VERSION }}

      - name: "Install helm (minimum and stable)"
        run: |
          wget https://get.helm.sh/helm-${HELM_VERSION_MIN}-linux-amd64.tar.gz
          tar zxf helm-$HELM_VERSION_MIN-linux-amd64.tar.gz
          sudo mv linux-amd64/helm /usr/local/bin/

          wget https://get.helm.sh/helm-${HELM_VERSION_STABLE}-linux-amd64.tar.gz
          tar zxf helm-$HELM_VERSION_STABLE-linux-amd64.tar.gz
          sudo mv linux-amd64/helm /usr/local/bin/helm-stable
      - name: Run chart template test
        run: ./script/chart-template-test.sh

  build_docker_images:
    name: "Build ${{matrix.image}} image"
    needs:
      - setup
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        image:
          - apprepository-controller
          - asset-syncer
          - kubeapps-apis
          - pinniped-proxy
    steps:
      - uses: docker/metadata-action@v4
        id: meta
        with:
          images: ${{needs.setup.outputs.img_prefix}}${{matrix.image}}${{needs.setup.outputs.img_modifier}}
          flavor: latest=true
          tags: ${{needs.setup.outputs.img_dev_tag}}
      - uses: docker/setup-qemu-action@v2
      - uses: docker/setup-buildx-action@v2
      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      - name: Build image
        uses: docker/build-push-action@v2
        with:
          file: cmd/${{matrix.image}}/Dockerfile
          platforms: ${{ env.IMG_PLATFORMS }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          outputs: type=docker,dest=/tmp/${{matrix.image}}-image.tar
      - name: Upload image
        uses: actions/upload-artifact@v3
        with:
          name: ${{matrix.image}}-image
          path: /tmp/${{matrix.image}}-image.tar
      - name: Push image
        uses: docker/build-push-action@v2
        with:
          file: cmd/${{matrix.image}}/Dockerfile
          platforms: ${{ env.IMG_PLATFORMS }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          push: true
          #TODO Conditionally push??

  build_dashboard_image:
    name: "Build dashboard image"
    needs:
      - setup
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: docker/metadata-action@v4
        id: meta
        with:
          images: ${{needs.setup.outputs.img_prefix}}dashboard${{needs.setup.outputs.img_modifier}}
          tags: ${{needs.setup.outputs.img_dev_tag}}
      - uses: docker/setup-qemu-action@v2
      - uses: docker/setup-buildx-action@v2
      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      - name: Build image
        uses: docker/build-push-action@v2
        with:
          context: dashboard
          platforms: ${{ env.IMG_PLATFORMS }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          outputs: type=docker,dest=/tmp/dashboard-image.tar
      - name: Upload image
        uses: actions/upload-artifact@v3
        with:
          name: dashboard-image
          path: /tmp/dashboard-image.tar
      - name: Push image
        uses: docker/build-push-action@v2
        with:
          context: dashboard
          platforms: ${{ env.IMG_PLATFORMS }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          push: true
          #TODO Conditionally push??

  build_e2e_runner_image:
    name: "Build E2E runner image"
    needs:
      - setup
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: docker/metadata-action@v4
        id: meta
        with:
          images: ${{needs.setup.outputs.img_prefix}}integration-tests${{needs.setup.outputs.img_modifier}}
          tags: ${{needs.setup.outputs.img_dev_tag}}
      - uses: docker/setup-buildx-action@v2
      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      - name: Build image
        uses: docker/build-push-action@v2
        with:
          context: integration
          # It doesn't make sense investing CI time in making a multiplatform image here
          platforms: linux/amd64
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          outputs: type=docker,dest=/tmp/e2e-runner-image.tar
      - name: Upload image
        uses: actions/upload-artifact@v3
        with:
          name: dashboard-image
          path: /tmp/e2e-runner-image.tar
      - name: Push image
        uses: docker/build-push-action@v2
        with:
          context: integration
          platforms: linux/amd64
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          push: true

  sync_chart_from_bitnami:
    needs:
      - setup
    runs-on: ubuntu-latest
    steps:
      - run: echo TODO
        if: false

  local_e2e_tests:
    needs:
      - setup
      - test_go
      - test_dashboard
      - test_pinniped_proxy
      - test_chart_render
      - build_docker_images
      - build_dashboard_image
      - build_e2e_runner_image
      - sync_chart_from_bitnami
    runs-on: ubuntu-latest
    env:
      DEFAULT_DEX_IP: "172.18.0.2"
      IMG_PREFIX: ${{ needs.setup.outputs.img_prefix }}
      TEST_UPGRADE: "1"
      USE_MULTICLUSTER_OIDC_ENV: "true"
      TEST_OPERATORS: "1"
      TEST_TIMEOUT_MINUTES: 4 # Timeout minutes for each test
    steps:
      - uses: actions/checkout@v3
      - name: "Install Kind"
        run: |
          curl -LO https://github.com/kubernetes-sigs/kind/releases/download/${KIND_VERSION}/kind-Linux-amd64
          chmod +x kind-Linux-amd64
          sudo mv kind-Linux-amd64 /usr/local/bin/kind
      - name: "Install kubectl"
        run: |
          curl -LO https://storage.googleapis.com/kubernetes-release/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl
          chmod +x ./kubectl
          sudo mv ./kubectl /usr/local/bin/kubectl
      - name: "Install cluster"
        run: |
          sed -i "s/172.18.0.2/$DEFAULT_DEX_IP/g" ./site/content/docs/latest/reference/manifests/kubeapps-local-dev-apiserver-config.yaml
          {
            echo "Creating cluster..."
            kind create cluster --image kindest/node:${K8S_KIND_VERSION} --name kubeapps-ci --config=./site/content/docs/latest/reference/manifests/kubeapps-local-dev-apiserver-config.yaml --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci --retain --wait 120s &&
            kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci apply -f ./site/content/docs/latest/reference/manifests/kubeapps-local-dev-users-rbac.yaml &&

            kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml &&
            sleep 5 &&
            kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci wait --namespace ingress-nginx --for=condition=ready pod --selector=app.kubernetes.io/component=controller --timeout=120s &&

            kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci create rolebinding kubeapps-view-secret-oidc --role view-secrets --user oidc:kubeapps-user@example.com &&
            kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci create clusterrolebinding kubeapps-view-oidc  --clusterrole=view --user oidc:kubeapps-user@example.com &&
            echo "Cluster created"
          } || {
            echo "Cluster creation failed, retrying..."
            kind delete clusters kubeapps-ci || true
            kind create cluster --image kindest/node:${K8S_KIND_VERSION} --name kubeapps-ci --config=./site/content/docs/latest/reference/manifests/kubeapps-local-dev-apiserver-config.yaml --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci --retain --wait 120s || true &&
            kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci apply -f ./site/content/docs/latest/reference/manifests/kubeapps-local-dev-users-rbac.yaml &&
            kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml &&
            sleep 5 &&
            kubectl wait --namespace ingress-nginx --for=condition=ready pod --selector=app.kubernetes.io/component=controller --timeout=120s &&

            kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci create rolebinding kubeapps-view-secret-oidc --role view-secrets --user oidc:kubeapps-user@example.com &&
            kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci create clusterrolebinding kubeapps-view-oidc  --clusterrole=view --user oidc:kubeapps-user@example.com &&
            echo "Cluster created"
          } || {
            echo "Error while creating the cluster after retry"
            exit 1
          }
      - name: "Copy apiserver certificates"
        run: |
          # dex will be running on the same node as the API server in the dev environment, so we can reuse the key and cert from the apiserver
          docker cp kubeapps-ci-control-plane:/etc/kubernetes/pki/apiserver.crt ./devel/dex.crt
          docker cp kubeapps-ci-control-plane:/etc/kubernetes/pki/apiserver.key ./devel/dex.key
          sudo chown $(whoami) ./devel/dex.key
          sudo chown $(whoami) ./devel/dex.crt
      - name: "Install additional cluster"
        run: |
          sed -i "s/172.18.0.2/$DEFAULT_DEX_IP/g" ./site/content/docs/latest/reference/manifests/kubeapps-local-dev-additional-apiserver-config.yaml
          {
            echo "Creating additional cluster..."
            kind create cluster --image kindest/node:${K8S_KIND_VERSION} --name kubeapps-ci-additional --config=./site/content/docs/latest/reference/manifests/kubeapps-local-dev-additional-apiserver-config.yaml --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci-additional --retain --wait 120s &&
            kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional apply --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci-additional -f ./site/content/docs/latest/reference/manifests/kubeapps-local-dev-users-rbac.yaml &&
            kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional apply --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci-additional -f ./site/content/docs/latest/reference/manifests/kubeapps-local-dev-namespace-discovery-rbac.yaml &&

            kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional create rolebinding kubeapps-view-secret-oidc --role view-secrets --user oidc:kubeapps-user@example.com &&
            kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional create clusterrolebinding kubeapps-view-oidc  --clusterrole=view --user oidc:kubeapps-user@example.com &&
            echo "Additional cluster created"
          } || {
            echo "Additional cluster creation failed, retrying..."
            kind delete clusters kubeapps-ci-additional || true
            kind create cluster --image kindest/node:${K8S_KIND_VERSION} --name kubeapps-ci-additional --config=./site/content/docs/latest/reference/manifests/kubeapps-local-dev-additional-apiserver-config.yaml --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci-additional --retain --wait 120s &&
            kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional apply --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci-additional -f ./site/content/docs/latest/reference/manifests/kubeapps-local-dev-users-rbac.yaml &&
            kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional apply --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci-additional -f ./site/content/docs/latest/reference/manifests/kubeapps-local-dev-namespace-discovery-rbac.yaml &&

            kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional create rolebinding kubeapps-view-secret-oidc --role view-secrets --user oidc:kubeapps-user@example.com &&
            kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional create clusterrolebinding kubeapps-view-oidc  --clusterrole=view --user oidc:kubeapps-user@example.com &&
            echo "Additional cluster created"
          } || {
            echo "Error while creating the additional cluster after retry"
          }
      - name: "Export cluster variables"
        run: |
          DEX_IP=`docker network inspect kind | jq '.[0].IPAM.Config[0].Gateway' | sed  's/"//g' | awk -F. '{ print $1"."$2"."$3"."$4+1 }'`
          ADDITIONAL_CLUSTER_IP=`docker network inspect kind | jq '.[0].IPAM.Config[0].Gateway' | sed  's/"//g' | awk -F. '{ print $1"."$2"."$3"."$4+2 }'`

          echo DEFAULT_DEX_IP=$DEFAULT_DEX_IP
          echo DEX_IP=$DEX_IP
          echo ADDITIONAL_CLUSTER_IP=$ADDITIONAL_CLUSTER_IP

          # If running kubectl without args, use the default "kubeapps-ci" cluster
          cp ${HOME}/.kube/kind-config-kubeapps-ci ${HOME}/.kube/config
          kubectl config set-context kind-kubeapps-ci

          # If the default IP the proper one, the multicluster setup will fail
          if [ "$DEFAULT_DEX_IP" != "$DEX_IP" ]; then echo "Default IP does not match with current IP used in Kind"; exit 1; fi

          echo "DEFAULT_DEX_IP=${DEFAULT_DEX_IP}" >> $GITHUB_ENV
          echo "DEX_IP=${DEX_IP}" >> $GITHUB_ENV
          echo "ADDITIONAL_CLUSTER_IP=${ADDITIONAL_CLUSTER_IP}" >> $GITHUB_ENV
      - name: "Install mkcert"
        run: |
          curl -LO "https://github.com/FiloSottile/mkcert/releases/download/${MKCERT_VERSION}/mkcert-${MKCERT_VERSION}-linux-amd64"
          chmod +x "mkcert-${MKCERT_VERSION}-linux-amd64"
          sudo mv "mkcert-${MKCERT_VERSION}-linux-amd64" /usr/local/bin/mkcert
          mkcert -install
      - name: "Install helm (minimum and stable)"
        run: |
          wget https://get.helm.sh/helm-${HELM_VERSION_MIN}-linux-amd64.tar.gz
          tar zxf helm-$HELM_VERSION_MIN-linux-amd64.tar.gz
          sudo mv linux-amd64/helm /usr/local/bin/

          wget https://get.helm.sh/helm-${HELM_VERSION_STABLE}-linux-amd64.tar.gz
          tar zxf helm-$HELM_VERSION_STABLE-linux-amd64.tar.gz
          sudo mv linux-amd64/helm /usr/local/bin/helm-stable
      - name: "Load needed images into Kind"
        run: |
          ./script/load-kind-image.sh docker.io/bitnami/apache:2.4.48-debian-10-r74 kubeapps-ci kubeapps-ci-additional &&
          ./script/load-kind-image.sh docker.io/bitnami/apache:2.4.48-debian-10-r75 kubeapps-ci kubeapps-ci-additional &&
          ./script/load-kind-image.sh registry:$DOCKER_REGISTRY_VERSION kubeapps-ci kubeapps-ci-additional
      - name: "Download docker images"
        uses: actions/download-artifact@v3
        with:
          path: /tmp/images
      - name: "Load CI images in the cluster"
        run: |
          for path in /tmp/images/*; do 
            image=$(basename "$path")
            kind load image-archive "${path}/${image}.tar" --name kubeapps-ci; 
          done
      - name: "Install multicluster deps"
        run: |
          sed -i -e "s/172.18.0.2/$DEFAULT_DEX_IP/g;s/localhost/kubeapps-ci.kubeapps/g" ./site/content/docs/latest/reference/manifests/kubeapps-local-dev-dex-values.yaml
          helm repo add dex https://charts.dexidp.io

          # Install dex
          kubectl create namespace dex
          helm install dex dex/dex --version 0.5.0 --namespace dex --values ./site/content/docs/latest/reference/manifests/kubeapps-local-dev-dex-values.yaml

          # Install openldap
          helm repo add stable https://charts.helm.sh/stable
          kubectl create namespace ldap
          helm install ldap stable/openldap --namespace ldap

          # Create certs
          kubectl -n dex create secret tls dex-web-server-tls --key ./devel/dex.key --cert ./devel/dex.crt
          mkcert -key-file ./devel/localhost-key.pem -cert-file ./devel/localhost-cert.pem localhost kubeapps-ci.kubeapps $DEFAULT_DEX_IP
      - name: "Run e2e tests script"
        run: |
          # If we want to test the latest version instead we override the image to be used
          if [[ -n "${TEST_LATEST_RELEASE:-}" ]]; then
            source ./script/chart_sync_utils.sh
            latest="$(latestReleaseTag)"
            IMG_DEV_TAG=${latest/v/}
            IMG_MODIFIER=""
          fi
          if IMG_PREFIX=${IMG_PREFIX} ./script/e2e-test.sh $USE_MULTICLUSTER_OIDC_ENV $OLM_VERSION $IMG_DEV_TAG $IMG_MODIFIER $TEST_TIMEOUT_MINUTES $DEFAULT_DEX_IP $ADDITIONAL_CLUSTER_IP $KAPP_CONTROLLER_VERSION $CHARTMUSEUM_VERSION; then
            echo "TEST_RESULT=0" >> $GITHUB_ENV
            exit 0
          else
            echo "TEST_RESULT=1" >> $GITHUB_ENV
            exit 1
          fi
      - name: "Print k8s KubeappsAPIs logs if the tests fail"
        run: kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci logs -n kubeapps-ci deploy/kubeapps-internal-kubeappsapis --previous=true
        if: failure() && env.TEST_RESULT == 1
      - name: 'Upload Artifacts'
        uses: actions/upload-artifact@v3
        with:
          name: integration_reports
          path: integration/reports

  push_images:
    runs-on: ubuntu-latest
    needs:
      - setup
      - local_e2e_tests
    env:
      IMG_PROD_TAG: ${{ needs.setup.outputs.img_prod_tag }}
    steps:
      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      - uses: actions/download-artifact@v3
      - run: |
          for path in ./*; do
            artifact=$(basename "$path")
          
            if [[ "${artifact}" != *"-image" ]]; then
              echo "::notice ::Skipping artifact ${artifact}, it's not a docker image"          
              continue
            fi
            
            image=${artifact/-image/}
            if [[ "${IMAGES_TO_PUSH}" != *"${image}"* ]]; then
              echo "::notice ::Skipping image ${image}, it's not an image to push"
              continue
            fi

            echo "::notice ::Loading image ${image}"
            docker load --input "${image}-image.tar"

            dev_image=${{needs.setup.outputs.img_prefix}}${image}${IMG_MODIFIER}:${IMG_DEV_TAG}
            prod_image=${{needs.setup.outputs.img_prefix}}${image}:${IMG_PROD_TAG}
            docker tag ${dev_image} ${prod_image}

            echo "::notice ::Pushing image ${prod_image}"
            docker push $prod_image
          done
  







