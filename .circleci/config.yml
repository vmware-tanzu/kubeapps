# Copyright 2018-2022 the Kubeapps contributors.
# SPDX-License-Identifier: Apache-2.0

version: 2.1
parameters:
  GOLANG_VERSION:
    type: string
    default: "1.17.6"
  NODE_VERSION:
    type: string
    default: "16.13.2"
  RUST_VERSION:
    type: string
    default: "1.58.1"
  DOCKER_VERSION:
    type: string
    default: "20.10.11"
  HELM_VERSION_MIN:
    type: "string"
    default: "v3.2.0"
  HELM_VERSION_STABLE:
    type: "string"
    default: "v3.8.0"
  OLM_VERSION:
    type: "string"
    default: "v0.20.0"
  MKCERT_VERSION:
    type: "string"
    default: "v1.4.3"
  KUBECTL_VERSION:
    type: "string"
    default: "v1.22.6"
  GITHUB_VERSION:
    type: "string"
    default: "2.4.0"
  SEMVER_VERSION:
    type: "string"
    default: "3.3.0"
  KIND_VERSION:
    type: "string"
    default: "v0.11.1"
  K8S_KIND_VERSION:
    type: "string"
    default: "v1.21.1@sha256:69860bda5563ac81e3c0057d654b5253219618a22ec3a346306239bba8cfa1a6"
  POSTGRESQL_VERSION:
    type: "string"
    default: "11.14.0-debian-10-r28"
  GKE_STABLE_VERSION:
    type: "string"
    default: "1.20"
  GKE_REGULAR_VERSION:
    type: "string"
    default: "1.21"
  IMAGES_TO_PUSH:
    type: "string"
    default: "kubeapps/apprepository-controller kubeapps/dashboard kubeapps/asset-syncer kubeapps/assetsvc kubeapps/kubeops kubeapps/pinniped-proxy kubeapps/kubeapps-apis"
  CI_BOT_USERNAME:
    type: "string"
    default: "kubeapps-bot"
  CI_BOT_EMAIL:
    type: "string"
    default: "tanzu-kubeapps-team@vmware.com"
  CI_BOT_GPG:
    type: "string"
    default: "80B6EB16B1328FB18DFF2A073EBA68F3347E319D"
  CI_BOT_KUBEAPPS_KUBEAPPS_DEPLOYKEY_FINGERPRINT:
    type: "string"
    default: "20:5e:d1:68:7c:10:67:c1:ab:e2:6e:33:e3:7a:64:6e"
  CI_BOT_KUBEAPPS_KUBEAPPS_DEPLOYKEY_FILENAME:
    type: "string"
    default: "id_rsa_205ed1687c1067c1abe26e33e37a646e"
  CI_BOT_FORKED_CHARTS_DEPLOYKEY_FINGERPRINT:
    type: "string"
    default: "59:81:2a:95:a6:54:a7:46:e5:15:f5:37:b3:4f:6c:ad"
  CI_BOT_FORKED_CHARTS_DEPLOYKEY_FILENAME:
    type: "string"
    default: "id_rsa_59812a95a654a746e515f537b34f6cad"
  CHARTS_REPO_ORIGINAL:
    type: "string"
    default: "bitnami/charts"
  BRANCH_CHARTS_REPO_ORIGINAL:
    type: "string"
    default: "master"
  CHARTS_REPO_FORKED:
    type: "string"
    default: "kubeapps-bot/charts"
  BRANCH_CHARTS_REPO_FORKED:
    type: "string"
    default: "master"
  KUBEAPPS_REPO:
    type: "string"
    default: "kubeapps/kubeapps"
  BRANCH_KUBEAPPS_REPO:
    type: "string"
    default: "main"

## Build conditions
# Build on any branch or tag
build_always: &build_always
  filters:
    tags:
      only: /^v.*/
# Build only on the main branch or in tags
build_on_main: &build_on_main
  filters:
    tags:
      only: /^v.*/
    branches:
      only: main
# Build only on tags (release)
build_on_tag: &build_on_tag
  filters:
    tags:
      only: /^v.*/
    branches:
      ignore: /.*/
# Build only on the prerelease branch or in tags (release)
build_on_tag_or_prerelease: &build_on_tag_or_prerelease
  filters:
    tags:
      only: /^v.*/
    branches:
      only: prerelease

workflows:
  version: 2
  kubeapps:
    jobs:
      - test_go:
          <<: *build_always
      - test_dashboard:
          <<: *build_always
      - test_pinniped_proxy:
          <<: *build_always
      - test_chart_render:
          <<: *build_always
      - build_go_images:
          <<: *build_always
      - build_dashboard:
          <<: *build_always
      - build_pinniped_proxy:
          <<: *build_always
      - local_e2e_tests:
          <<: *build_always
          matrix:
            parameters:
              # Enable the line below to do bundle builds to debug CI issues
              number: ["1"]
              # number: ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"]
          requires:
            - test_go
            - test_dashboard
            - test_pinniped_proxy
            - test_chart_render
            - build_go_images
            - build_dashboard
            - build_pinniped_proxy
            - sync_chart_from_bitnami
      - sync_chart_from_bitnami:
          <<: *build_on_main
      - GKE_REGULAR_VERSION_MAIN:
          <<: *build_on_tag_or_prerelease
          requires:
            - test_go
            - test_dashboard
            - test_pinniped_proxy
            - test_chart_render
            - build_go_images
            - build_dashboard
            - build_pinniped_proxy
            - sync_chart_from_bitnami
      - GKE_REGULAR_VERSION_LATEST_RELEASE:
          <<: *build_on_tag_or_prerelease
          requires:
            - test_go
            - test_dashboard
            - test_pinniped_proxy
            - test_chart_render
            - build_go_images
            - build_dashboard
            - build_pinniped_proxy
            - sync_chart_from_bitnami
      - GKE_STABLE_VERSION_MAIN:
          <<: *build_on_tag_or_prerelease
          requires:
            - test_go
            - test_dashboard
            - test_pinniped_proxy
            - test_chart_render
            - build_go_images
            - build_dashboard
            - build_pinniped_proxy
            - sync_chart_from_bitnami
      - GKE_STABLE_VERSION_LATEST_RELEASE:
          <<: *build_on_tag_or_prerelease
          requires:
            - test_go
            - test_dashboard
            - test_pinniped_proxy
            - test_chart_render
            - build_go_images
            - build_dashboard
            - build_pinniped_proxy
            - sync_chart_from_bitnami
      - push_images:
          <<: *build_on_main
          requires:
            - local_e2e_tests
      - sync_chart_to_bitnami:
          <<: *build_on_tag
          requires:
            - local_e2e_tests
            - GKE_REGULAR_VERSION_MAIN
            - GKE_REGULAR_VERSION_LATEST_RELEASE
            - GKE_STABLE_VERSION_MAIN
            - GKE_STABLE_VERSION_LATEST_RELEASE
      - release:
          <<: *build_on_tag
          requires:
            - sync_chart_to_bitnami
            - local_e2e_tests
            - GKE_REGULAR_VERSION_MAIN
            - GKE_REGULAR_VERSION_LATEST_RELEASE
            - GKE_STABLE_VERSION_MAIN
            - GKE_STABLE_VERSION_LATEST_RELEASE

## Definitions
common_envars: &common_envars
  DOCKER_VERSION: << pipeline.parameters.DOCKER_VERSION >>
  GOLANG_VERSION: << pipeline.parameters.GOLANG_VERSION >>
  HELM_VERSION_MIN: << pipeline.parameters.HELM_VERSION_MIN >>
  HELM_VERSION_STABLE: << pipeline.parameters.HELM_VERSION_STABLE >>
  K8S_KIND_VERSION: << pipeline.parameters.K8S_KIND_VERSION >>
  KIND_VERSION: << pipeline.parameters.KIND_VERSION >>
  KUBECTL_VERSION: << pipeline.parameters.KUBECTL_VERSION >>
  GITHUB_VERSION: << pipeline.parameters.GITHUB_VERSION >>
  SEMVER_VERSION: << pipeline.parameters.SEMVER_VERSION >>
  MKCERT_VERSION: << pipeline.parameters.MKCERT_VERSION >>
  NODE_VERSION: << pipeline.parameters.NODE_VERSION >>
  OLM_VERSION: << pipeline.parameters.OLM_VERSION >>
  POSTGRESQL_VERSION: << pipeline.parameters.POSTGRESQL_VERSION >>
  RUST_VERSION: << pipeline.parameters.RUST_VERSION >>

install_gcloud_sdk: &install_gcloud_sdk
  run:
    name: "Install gcloud sdk"
    command: |
      echo "export PATH=$PATH:${HOME}/google-cloud-sdk/bin" >> $BASH_ENV
      echo "export CLOUDSDK_CORE_DISABLE_PROMPTS=1" >> $BASH_ENV
      if [ ! -d ${HOME}/google-cloud-sdk/bin ]; then
        rm -rf $HOME/google-cloud-sdk;
        curl https://sdk.cloud.google.com | bash;
      fi
install_helm_cli: &install_helm_cli
  run:
    name: "Install helm (minimum and stable)"
    command: |
      wget https://get.helm.sh/helm-${HELM_VERSION_MIN}-linux-amd64.tar.gz
      tar zxf helm-$HELM_VERSION_MIN-linux-amd64.tar.gz
      sudo mv linux-amd64/helm /usr/local/bin/

      wget https://get.helm.sh/helm-${HELM_VERSION_STABLE}-linux-amd64.tar.gz
      tar zxf helm-$HELM_VERSION_STABLE-linux-amd64.tar.gz
      sudo mv linux-amd64/helm /usr/local/bin/helm-stable
exports: &exports
  run:
    name: "Export variables"
    command: |
      # It is not possible to resolve env vars in the environment section:
      # https://circleci.com/docs/2.0/env-vars/#using-bash_env-to-set-environment-variables
      # DEV_TAG and PROD_TAG are the tags used for the Kubeapps docker images
      echo "export DEV_TAG=build-${CIRCLE_SHA1}" >> $BASH_ENV
      echo "export PROD_TAG=${CIRCLE_TAG:-latest}" >> $BASH_ENV
      # Apart from using a DEV_TAG we use a different image ID to avoid polluting the tag
      # history of the production tag
      echo "export IMG_MODIFIER=-ci" >> $BASH_ENV
build_images: &build_images
  steps:
    - setup_remote_docker:
        version: << pipeline.parameters.DOCKER_VERSION >>
    - checkout
    - <<: *exports
    - run:
        name: Build and push CI images
        command: |
          mkdir -p images/
          read -ra IMG_ARRAY \<<< "$IMAGES"
          if [[ -n "${CIRCLE_TAG}" ]]; then
            makeArgs="VERSION=${CIRCLE_TAG}"
          fi
          for IMAGE in "${IMG_ARRAY[@]}"; do
            make IMG_MODIFIER="$IMG_MODIFIER" IMAGE_TAG="${DEV_TAG}" $makeArgs kubeapps/${IMAGE}
            if [[ -n "${DOCKER_USERNAME}" && -n "${DOCKER_PASSWORD}" ]]; then
              docker login -u="${DOCKER_USERNAME}" -p="${DOCKER_PASSWORD}"
              docker push kubeapps/${IMAGE}${IMG_MODIFIER}:${DEV_TAG}
            fi
            docker save kubeapps/${IMAGE}${IMG_MODIFIER}:${DEV_TAG} > images/${IMAGE}${IMG_MODIFIER}:${DEV_TAG}
          done
    - persist_to_workspace:
        root: images
        paths:
          - "*"
install_cluster: &install_cluster
  run:
    name: "Install cluster"
    command: |
      sed -i "s/172.18.0.2/$DEFAULT_DEX_IP/g" ./docs/user/manifests/kubeapps-local-dev-apiserver-config.yaml
      {
        echo "Creating cluster..."
        kind create cluster --image kindest/node:${K8S_KIND_VERSION} --name kubeapps-ci --config=./docs/user/manifests/kubeapps-local-dev-apiserver-config.yaml --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci --retain --wait 120s &&
        kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci apply -f ./docs/user/manifests/kubeapps-local-dev-users-rbac.yaml &&

        kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci apply -f ./docs/user/manifests/ingress-nginx-kind-with-large-proxy-buffers.yaml &&
        sleep 5 &&
        kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci wait --namespace ingress-nginx --for=condition=ready pod --selector=app.kubernetes.io/component=controller --timeout=120s &&

        kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci delete rolebinding kubeapps-user -n  kubeapps-user-namespace &&
        kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci create rolebinding kubeapps-view-secret-oidc --role view-secrets --user oidc:kubeapps-user@example.com &&
        kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci create clusterrolebinding kubeapps-view-oidc  --clusterrole=view --user oidc:kubeapps-user@example.com &&
        echo "Cluster created"
      } || {
        echo "Cluster creation failed, retrying..."
        kind delete clusters kubeapps-ci || true
        kind create cluster --image kindest/node:${K8S_KIND_VERSION} --name kubeapps-ci --config=./docs/user/manifests/kubeapps-local-dev-apiserver-config.yaml --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci --retain --wait 120s || true &&
        kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci apply -f ./docs/user/manifests/kubeapps-local-dev-users-rbac.yaml &&
        kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci apply -f ./docs/user/manifests/ingress-nginx-kind-with-large-proxy-buffers.yaml &&
        sleep 5 &&
        kubectl wait --namespace ingress-nginx --for=condition=ready pod --selector=app.kubernetes.io/component=controller --timeout=120s &&

        kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci delete rolebinding kubeapps-user -n  kubeapps-user-namespace &&
        kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci create rolebinding kubeapps-view-secret-oidc --role view-secrets --user oidc:kubeapps-user@example.com &&
        kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci create clusterrolebinding kubeapps-view-oidc  --clusterrole=view --user oidc:kubeapps-user@example.com &&
        echo "Cluster created"
      } || {
        echo "Error while creating the cluster after retry"
      }
export_cluster_variables: &export_cluster_variables
  run:
    name: "Export cluster variables"
    command: |
      DEX_IP=`docker network inspect kind | jq '.[0].IPAM.Config[0].Gateway' | sed  's/"//g' | awk -F. '{ print $1"."$2"."$3"."$4+1 }'`
      ADDITIONAL_CLUSTER_IP=`docker network inspect kind | jq '.[0].IPAM.Config[0].Gateway' | sed  's/"//g' | awk -F. '{ print $1"."$2"."$3"."$4+2 }'`

      echo DEFAULT_DEX_IP=$DEFAULT_DEX_IP
      echo DEX_IP=$DEX_IP
      echo ADDITIONAL_CLUSTER_IP=$ADDITIONAL_CLUSTER_IP

      # If running kubectl without args, use the default "kubeapps-ci" cluster
      cp ${HOME}/.kube/kind-config-kubeapps-ci ${HOME}/.kube/config
      kubectl config set-context kind-kubeapps-ci

      # If the default IP the proper one, the multicluster setup will fail
      if [ "$DEFAULT_DEX_IP" != "$DEX_IP" ]; then echo "Default IP does not match with current IP used in Kind"; exit 1; fi

      echo "export DEFAULT_DEX_IP=${DEFAULT_DEX_IP}" >> $BASH_ENV
      echo "export DEX_IP=${DEX_IP}" >> $BASH_ENV
      echo "export ADDITIONAL_CLUSTER_IP=${ADDITIONAL_CLUSTER_IP}" >> $BASH_ENV
install_additional_cluster: &install_additional_cluster
  run:
    name: "Install additional cluster"
    command: |
      sed -i "s/172.18.0.2/$DEFAULT_DEX_IP/g" ./docs/user/manifests/kubeapps-local-dev-additional-apiserver-config.yaml
      {
        echo "Creating additional cluster..."
        kind create cluster --image kindest/node:${K8S_KIND_VERSION} --name kubeapps-ci-additional --config=./docs/user/manifests/kubeapps-local-dev-additional-apiserver-config.yaml --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci-additional --retain --wait 120s &&
        kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional apply --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci-additional -f ./docs/user/manifests/kubeapps-local-dev-users-rbac.yaml &&
        kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional apply --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci-additional -f ./docs/user/manifests/kubeapps-local-dev-namespace-discovery-rbac.yaml &&

        kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional delete rolebinding kubeapps-user -n  kubeapps-user-namespace &&
        kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional create rolebinding kubeapps-view-secret-oidc --role view-secrets --user oidc:kubeapps-user@example.com &&
        kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional create clusterrolebinding kubeapps-view-oidc  --clusterrole=view --user oidc:kubeapps-user@example.com &&
        echo "Additional cluster created"
      } || {
        echo "Additional cluster creation failed, retrying..."
        kind delete clusters kubeapps-ci-additional || true
        kind create cluster --image kindest/node:${K8S_KIND_VERSION} --name kubeapps-ci-additional --config=./docs/user/manifests/kubeapps-local-dev-additional-apiserver-config.yaml --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci-additional --retain --wait 120s &&
        kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional apply --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci-additional -f ./docs/user/manifests/kubeapps-local-dev-users-rbac.yaml &&
        kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional apply --kubeconfig=${HOME}/.kube/kind-config-kubeapps-ci-additional -f ./docs/user/manifests/kubeapps-local-dev-namespace-discovery-rbac.yaml &&

        kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional delete rolebinding kubeapps-user -n  kubeapps-user-namespace &&
        kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional create rolebinding kubeapps-view-secret-oidc --role view-secrets --user oidc:kubeapps-user@example.com &&
        kubectl --context kind-kubeapps-ci-additional --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci-additional create clusterrolebinding kubeapps-view-oidc  --clusterrole=view --user oidc:kubeapps-user@example.com &&
        echo "Additional cluster created"
      } || {
        echo "Error while creating the additional cluster after retry"
      }
load_kind_images: &load_kind_images
  run:
    name: "Load needed images into Kind"
    command:
      ./script/load-kind-image.sh docker.io/bitnami/apache:2.4.48-debian-10-r74 kubeapps-ci kubeapps-ci-additional &&
      ./script/load-kind-image.sh docker.io/bitnami/apache:2.4.48-debian-10-r75 kubeapps-ci kubeapps-ci-additional
copy_apiserver_certificates: &copy_apiserver_certificates
  run:
    name: "Copy apiserver certificates"
    command: |
      # dex will be running on the same node as the API server in the dev environment, so we can reuse the key and cert from the apiserver
      docker cp kubeapps-ci-control-plane:/etc/kubernetes/pki/apiserver.crt ./devel/dex.crt
      docker cp kubeapps-ci-control-plane:/etc/kubernetes/pki/apiserver.key ./devel/dex.key
      sudo chown circleci ./devel/dex.key
      sudo chown circleci ./devel/dex.crt
install_kubectl: &install_kubectl
  run:
    name: "Install kubectl"
    command: |
      curl -LO https://storage.googleapis.com/kubernetes-release/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl
      chmod +x ./kubectl
      sudo mv ./kubectl /usr/local/bin/kubectl
install_kind: &install_kind
  run:
    name: "Install kind"
    command: |
      curl -LO https://github.com/kubernetes-sigs/kind/releases/download/${KIND_VERSION}/kind-Linux-amd64
      chmod +x kind-Linux-amd64
      sudo mv kind-Linux-amd64 /usr/local/bin/kind
install_mkcert: &install_mkcert
  run:
    name: "Install mkcert"
    command: |
      curl -LO "https://github.com/FiloSottile/mkcert/releases/download/${MKCERT_VERSION}/mkcert-${MKCERT_VERSION}-linux-amd64"
      chmod +x "mkcert-${MKCERT_VERSION}-linux-amd64"
      sudo mv "mkcert-${MKCERT_VERSION}-linux-amd64" /usr/local/bin/mkcert
      mkcert -install
install_github: &install_github
  run:
    # Assuming there is a personal access token created in GitHub granted with the scopes
    # "repo:status", "public_repo" and "read:org"
    # This token is passed as a GITHUB_TOKEN env var via CircleCI
    name: "Install GitHub CLI"
    command: |
      cd /tmp
      wget https://github.com/cli/cli/releases/download/v${GITHUB_VERSION}/gh_${GITHUB_VERSION}_linux_amd64.tar.gz
      tar zxf gh_${GITHUB_VERSION}_linux_amd64.tar.gz
      rm gh_${GITHUB_VERSION}_linux_amd64.tar.gz
      sudo mv gh_${GITHUB_VERSION}_linux_amd64/bin/gh /usr/local/bin/
install_semver: &install_semver
  run:
    name: "Install semver bash tool"
    command: |
      cd /tmp
      wget https://github.com/fsaintjacques/semver-tool/archive/refs/tags/${SEMVER_VERSION}.tar.gz
      tar zxf ${SEMVER_VERSION}.tar.gz
      rm ${SEMVER_VERSION}.tar.gz
      cd semver-tool-${SEMVER_VERSION}
      sudo make install
setup_gpg: &setup_gpg
  run:
    name: "Install the GPG key"
    command: |
      # Creating the files from the GPG_KEY_PUBLIC and GPG_KEY_PRIVATE env vars
      echo -e ${GPG_KEY_PUBLIC} > /tmp/public.key
      echo -e ${GPG_KEY_PRIVATE} > /tmp/private.key

      # Importing the GPG keys
      gpg --import /tmp/public.key
      gpg --import --no-tty --batch --yes /tmp/private.key

      # Trusting the imported GPG private key
      (echo 5; echo y; echo save) |  gpg --command-fd 0 --no-tty --no-greeting -q --edit-key "<< pipeline.parameters.CI_BOT_GPG >>" trust

      # Listing the key to verify the import process succeeded
      gpg --list-secret-keys << pipeline.parameters.CI_BOT_EMAIL >>

install_multicluster_deps: &install_multicluster_deps
  run:
    name: "Install multicluster deps"
    command: |
      sed -i -e "s/172.18.0.2/$DEFAULT_DEX_IP/g;s/localhost/kubeapps-ci.kubeapps/g" ./docs/user/manifests/kubeapps-local-dev-dex-values.yaml
      helm repo add dex https://charts.dexidp.io

      # Install dex
      kubectl create namespace dex
      helm install dex dex/dex --version 0.5.0 --namespace dex --values ./docs/user/manifests/kubeapps-local-dev-dex-values.yaml

      # Install openldap
      helm repo add stable https://charts.helm.sh/stable
      kubectl create namespace ldap
      helm install ldap stable/openldap --namespace ldap

      # Create certs
      kubectl -n dex create secret tls dex-web-server-tls --key ./devel/dex.key --cert ./devel/dex.crt
      mkcert -key-file ./devel/localhost-key.pem -cert-file ./devel/localhost-cert.pem localhost kubeapps-ci.kubeapps $DEFAULT_DEX_IP

run_e2e_tests: &run_e2e_tests
  run:
    name: "Run e2e tests script"
    command: |
      # If we want to test the latest version instead we override the image to be used
      if [[ -n "${TEST_LATEST_RELEASE:-}" ]]; then
        source ./script/chart_sync_utils.sh
        latest="$(latestReleaseTag)"
        DEV_TAG=${latest/v/}
        IMG_MODIFIER=""
      fi
      if ./script/e2e-test.sh $USE_MULTICLUSTER_OIDC_ENV $OLM_VERSION $DEV_TAG $IMG_MODIFIER $DEFAULT_DEX_IP $ADDITIONAL_CLUSTER_IP; then
        # Test success
        echo "export TEST_RESULT=$?" >> $BASH_ENV
      else
        # Test failed
        echo "export TEST_RESULT=$?" >> $BASH_ENV
      fi
gke_test: &gke_test
  docker:
    - image: cimg/go:<< pipeline.parameters.GOLANG_VERSION >>
  steps:
    - checkout
    - run:
        name: Check conditions
        command: |
          source ./script/chart_sync_utils.sh

          # In the case of GKE we will only want to build if it is
          # a build of a branch in the kubeapps repository
          if [[ -z "$GKE_ADMIN" ]]; then
            echo "Step cancelled, we are not in the Kubeapps repository"
            circleci step halt
          fi

          # Cancel job if this is a test stable release job but
          # the chart version has not been bumped
          if [[ -n "${TEST_LATEST_RELEASE:-}" ]] && ! changedVersion; then
            echo "Step cancelled, we are not releasing a new version of the chart"
            circleci step halt
          fi
    - <<: *exports
    - <<: *install_gcloud_sdk
    - setup_remote_docker:
          version: << pipeline.parameters.DOCKER_VERSION >>
    - run:
        name: Configure Google Cloud
        command: |
          gcloud -q config set project $GKE_PROJECT
          export GOOGLE_APPLICATION_CREDENTIALS=/tmp/client_secrets.json
          echo $GCLOUD_KEY > $GOOGLE_APPLICATION_CREDENTIALS
          if [ -a $GOOGLE_APPLICATION_CREDENTIALS ]; then
            gcloud -q auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS;
          fi
    - <<: *install_kubectl
    # A GKE cluster name cannot contain non-alphanumeric characters (nor uppercase letters)
    - run:
        name: Export escaped GKE cluster name
        command: |
          echo "export ESCAPED_GKE_CLUSTER=$(echo ${GKE_CLUSTER}-${CIRCLE_BRANCH:-$CIRCLE_TAG}-${TEST_LATEST_RELEASE}-${GKE_BRANCH}-ci | sed 's/[^a-z0-9-]//g')" >> $BASH_ENV
    - run:
        name: Start GKE environment
        command: |
          ./script/start-gke-env.sh $ESCAPED_GKE_CLUSTER $GKE_ZONE $GKE_BRANCH $GKE_ADMIN > /dev/null
    - <<: *install_helm_cli
    - <<: *run_e2e_tests
    - store_artifacts:
        path: integration/reports
    - run: exit $TEST_RESULT
    - run:
        name: Cleanup GKE Cluster
        command: gcloud container clusters delete --async --zone $GKE_ZONE $ESCAPED_GKE_CLUSTER
        when: always
local_e2e_steps: &local_e2e_steps
  steps:
    - checkout
    - <<: *exports
    - <<: *install_kind
    - <<: *install_kubectl
    - <<: *install_cluster
    - <<: *copy_apiserver_certificates
    # Create the "kubeapps-ci-additional" cluster
    - <<: *install_additional_cluster
    # Export variables and kubeconfig
    - <<: *export_cluster_variables
    - <<: *install_mkcert
    - <<: *install_helm_cli
    - <<: *load_kind_images
    # Load images from other jobs
    - attach_workspace:
        at: /tmp/images
    - run:
        name: Load CI images in the cluster
        command: for image in /tmp/images/*; do kind load image-archive "$image" --name kubeapps-ci; done
    - <<: *install_multicluster_deps
    - <<: *run_e2e_tests
    - run:
        name: Print k8s KubeappsAPIs logs if the tests fail
        command: |
          kubectl --context kind-kubeapps-ci --kubeconfig ${HOME}/.kube/kind-config-kubeapps-ci logs -n kubeapps-ci deploy/kubeapps-internal-kubeappsapis --previous=true
        when: on_fail
    - store_artifacts:
        path: integration/reports
    - run: exit $TEST_RESULT
###

jobs:
  test_go:
    working_directory: /home/circleci/go/src/github.com/kubeapps/kubeapps
    environment:
      CGO_ENABLED: "0"
      <<: *common_envars
    docker:
      - image: cimg/go:<< pipeline.parameters.GOLANG_VERSION >>
    steps:
      - checkout
      - <<: *exports
      - run:
          name: Run go unit tests
          command: |
            make test
      - setup_remote_docker:
          version: << pipeline.parameters.DOCKER_VERSION >>
      - run:
          name: Run go integration tests for DB
          command: |
            docker run -d --name postgresql --rm --publish 5432:5432 -e ALLOW_EMPTY_PASSWORD=yes bitnami/postgresql:${POSTGRESQL_VERSION}
            docker run --network container:postgresql -d --name tests cimg/go:${GOLANG_VERSION} tail -f /dev/null
            docker cp /home/circleci/go tests:/
            docker exec -it tests /bin/sh -c "cd /go/src/github.com/kubeapps/kubeapps/ && make test-db"
  test_dashboard:
    environment:
      # Note that the max old space setting is per worker, so running the tests
      # with 4 workers on a 4Gb (free plan) needs 1Gb of max old space. Forcing
      # garbage collection to start earlier with 512M per worker.
      NODE_OPTIONS: "--max-old-space-size=512"
    docker:
      - image: cimg/node:<< pipeline.parameters.NODE_VERSION >>
    steps:
      - checkout
      - run:
          name: Install dashboard dependencies
          command: |
            yarn install --cwd=dashboard --frozen-lockfile
      - run:
          name: Run dashboard linter
          command: |
            yarn --cwd=dashboard run lint
      - run:
          name: Run dashboard unit tests
          command: |
            yarn --cwd=dashboard run test --maxWorkers=4 --coverage --logHeapUsage
  test_pinniped_proxy:
    docker:
      - image: cimg/rust:<< pipeline.parameters.RUST_VERSION >>
    steps:
      - checkout
      - run:
          name: Run rust unit tests
          command: |
            cargo test --manifest-path ./cmd/pinniped-proxy/Cargo.toml
  test_chart_render:
    environment:
      <<: *common_envars
    docker:
      - image: cimg/go:<< pipeline.parameters.GOLANG_VERSION >>
    steps:
      - <<: *exports
      - checkout
      - <<: *install_helm_cli
      - run:
          name: Run chart template test
          command: |
            ./script/chart-template-test.sh
  build_go_images:
    docker:
      - image: cimg/go:<< pipeline.parameters.GOLANG_VERSION >>
    working_directory: /home/circleci/go/src/github.com/kubeapps/kubeapps
    environment:
      GOPATH: ${HOME}/.go_workspace
      IMAGES: "kubeops apprepository-controller asset-syncer assetsvc kubeapps-apis"
    <<: *build_images
  build_dashboard:
    docker:
      - image: cimg/go:<< pipeline.parameters.GOLANG_VERSION >>
    environment:
      IMAGES: "dashboard"
    <<: *build_images
  build_pinniped_proxy:
    docker:
      # We're building the image in a docker container anyway so just re-use the golang image already in use.
      - image: cimg/go:<< pipeline.parameters.GOLANG_VERSION >>
    environment:
      IMAGES: "pinniped-proxy"
    <<: *build_images
  release:
    environment:
      <<: *common_envars
    docker:
      - image: cimg/go:<< pipeline.parameters.GOLANG_VERSION >>
    steps:
      - checkout
      - <<: *install_github
      - run:
          # Assuming there is a personal access token created in GitHub granted with the scopes
          # "repo:status", "public_repo" and "read:org"
          # This token is passed as a GITHUB_TOKEN env var via CircleCI
          name: Run the create_release script
          command: |
            ./script/create_release.sh ${CIRCLE_TAG} << pipeline.parameters.KUBEAPPS_REPO >>
  local_e2e_tests:
    machine: true
    # Reference for available machines
    # https://circleci.com/docs/2.0/configuration-reference/#machine-executor-linux
    resource_class: large
    environment:
      DEFAULT_DEX_IP: "172.18.0.2"
      TEST_UPGRADE: "1"
      USE_MULTICLUSTER_OIDC_ENV: "true"
      TEST_OPERATORS: "1"
      <<: *common_envars
    parameters:
      number:
        type: string
    <<: *local_e2e_steps
  GKE_REGULAR_VERSION_MAIN:
    <<: *gke_test
    environment:
      GKE_BRANCH: << pipeline.parameters.GKE_REGULAR_VERSION >>
      USE_MULTICLUSTER_OIDC_ENV: "false"
      <<: *common_envars
  GKE_REGULAR_VERSION_LATEST_RELEASE:
    <<: *gke_test
    environment:
      GKE_BRANCH: << pipeline.parameters.GKE_REGULAR_VERSION >>
      TEST_LATEST_RELEASE: 1
      USE_MULTICLUSTER_OIDC_ENV: "false"
      <<: *common_envars
  GKE_STABLE_VERSION_MAIN:
    <<: *gke_test
    environment:
      GKE_BRANCH: << pipeline.parameters.GKE_STABLE_VERSION >>
      USE_MULTICLUSTER_OIDC_ENV: "false"
      <<: *common_envars
  GKE_STABLE_VERSION_LATEST_RELEASE:
    <<: *gke_test
    environment:
      GKE_BRANCH: << pipeline.parameters.GKE_STABLE_VERSION >>
      TEST_LATEST_RELEASE: 1
      USE_MULTICLUSTER_OIDC_ENV: "false"
      <<: *common_envars
  sync_chart_to_bitnami:
    environment:
      <<: *common_envars
    docker:
      - image: cimg/go:<< pipeline.parameters.GOLANG_VERSION >>
    steps:
      - checkout
      - <<: *install_github
      - <<: *install_semver
      - <<: *setup_gpg
      - add_ssh_keys:
          fingerprints:
            # Deployment key uploaded to the kubeapps-bot/charts repository
            - << pipeline.parameters.CI_BOT_FORKED_CHARTS_DEPLOYKEY_FINGERPRINT >>

      - run:
          # This is a key pair: https://circleci.com/docs/2.0/gh-bb-integration/
          # public key uploaded to GitHub as a deploy key with write permissions in both kubeapps and kubeapps-bot/charts
          # private key uploaded to CircleCI with hostname "github.com"
          name: Start ssh-agent and configure the key
          command: |
            eval "$(ssh-agent -s)"
            # the name is always "id_rsa_"+fingerprint without ":""
            # Deployment key uploaded to the kubeapps-bot/charts repository
            ssh-add ~/.ssh/<< pipeline.parameters.CI_BOT_FORKED_CHARTS_DEPLOYKEY_FILENAME >>
      - run:
          # Assuming there is a personal access token created in GitHub granted with the scopes
          # "repo:status", "public_repo" and "read:org"
          # This token is passed as a GITHUB_TOKEN env var via CircleCI
          name: Run the chart_sync script
          command: |
            ./script/chart_sync.sh << pipeline.parameters.CI_BOT_USERNAME >> << pipeline.parameters.CI_BOT_EMAIL >> << pipeline.parameters.CI_BOT_GPG >> << pipeline.parameters.CHARTS_REPO_ORIGINAL >> << pipeline.parameters.BRANCH_CHARTS_REPO_ORIGINAL >> << pipeline.parameters.CHARTS_REPO_FORKED >> << pipeline.parameters.BRANCH_CHARTS_REPO_FORKED >>
  sync_chart_from_bitnami:
    environment:
      <<: *common_envars
    docker:
      - image: cimg/go:<< pipeline.parameters.GOLANG_VERSION >>
    steps:
      - checkout
      - <<: *install_github
      - <<: *install_semver
      - <<: *setup_gpg
      - add_ssh_keys:
          fingerprints:
            # Deployment key uploaded to the kubeapps/kubeapps repository
            - << pipeline.parameters.CI_BOT_KUBEAPPS_KUBEAPPS_DEPLOYKEY_FINGERPRINT >>
            # Deployment key uploaded to the kubeapps-bot/charts repository
            - << pipeline.parameters.CI_BOT_FORKED_CHARTS_DEPLOYKEY_FINGERPRINT >>
      - run:
          # This is a key pair: https://circleci.com/docs/2.0/gh-bb-integration/
          # public key uploaded to GitHub as a deploy key with write permissions
          # private key uploaded to CircleCI with hostname "github.com"
          name: Start ssh-agent and configure the key
          command: |
            eval "$(ssh-agent -s)"
            # the name is always "id_rsa_"+fingerprint without ":""
            # Deployment key uploaded to the kubeapps/kubeapps repository
            ssh-add ~/.ssh/<< pipeline.parameters.CI_BOT_KUBEAPPS_KUBEAPPS_DEPLOYKEY_FILENAME >>
            # Deployment key uploaded to the kubeapps-bot/charts repository
            ssh-add ~/.ssh/<< pipeline.parameters.CI_BOT_FORKED_CHARTS_DEPLOYKEY_FILENAME >>
      - run:
          # Assuming there is a personal access token created in GitHub granted with the scopes
          # "repo:status", "public_repo" and "read:org"
          # This token is passed as a GITHUB_TOKEN env var via CircleCI
          name: Run the check_upstream_chart script
          command: |
            ./script/chart_upstream_checker.sh << pipeline.parameters.CI_BOT_USERNAME >> << pipeline.parameters.CI_BOT_EMAIL >> << pipeline.parameters.CI_BOT_GPG >> << pipeline.parameters.CI_BOT_FORKED_CHARTS_DEPLOYKEY_FILENAME >> << pipeline.parameters.CHARTS_REPO_ORIGINAL >> << pipeline.parameters.BRANCH_CHARTS_REPO_ORIGINAL >> << pipeline.parameters.CHARTS_REPO_FORKED >> << pipeline.parameters.BRANCH_CHARTS_REPO_FORKED >> << pipeline.parameters.KUBEAPPS_REPO >> << pipeline.parameters.BRANCH_KUBEAPPS_REPO >>
  push_images:
    docker:
      - image: cimg/go:<< pipeline.parameters.GOLANG_VERSION >>
    steps:
      - setup_remote_docker:
          version: << pipeline.parameters.DOCKER_VERSION >>
      - <<: *exports
      - run:
          name: Push images
          command: |
            if [[ -z "$CIRCLE_PULL_REQUEST" && -n "$DOCKER_USERNAME" && -n "$DOCKER_PASSWORD" ]]; then
              docker login -u="${DOCKER_USERNAME}" -p="${DOCKER_PASSWORD}"
              for IMAGE in << pipeline.parameters.IMAGES_TO_PUSH >>; do
                docker pull ${IMAGE}${IMG_MODIFIER}:${DEV_TAG}
                docker tag ${IMAGE}${IMG_MODIFIER}:${DEV_TAG} ${IMAGE}:${PROD_TAG}
                docker push ${IMAGE}:${PROD_TAG}
              done
            fi
